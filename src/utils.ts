import { getCachedLogChannel } from "./state";
import { config } from "./config";

/**
 * OpenAI chat completionäº’æ›APIã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‹
 */
type ChatCompletionResponse = {
  choices?: Array<{
    message?: {
      content?: string;
    };
  }>;
};

/**
 * æ—¥æœ¬æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
 */
export function getJapaneseTimestamp(): string {
  const now = new Date();
  return now.toLocaleString("ja-JP", {
    timeZone: "Asia/Tokyo",
    year: "numeric",
    month: "2-digit",
    day: "2-digit",
    hour: "2-digit",
    minute: "2-digit",
    second: "2-digit",
  });
}

/**
 * OpenAI chat completionäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
 */
async function sendChatCompletionRequest(
  transcript: string
): Promise<string | null> {
  // ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆURLã¨APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
  if (!config.CHAT_COMPLETION_ENDPOINT_URL || !config.CHAT_COMPLETION_APIKEY) {
    if (config.VERBOSE) {
      console.log(
        "[LLM] Chat completion endpoint or API key not configured, skipping LLM processing"
      );
    }
    return null;
  }

  // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šï¼ˆ60ç§’ï¼‰
  const controller = new AbortController();
  const timeoutId = setTimeout(() => controller.abort(), 60000);

  try {
    // VERBOSEãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã‚’ãƒ­ã‚°å‡ºåŠ›
    if (config.VERBOSE) {
      console.log(
        `[LLM] Sending request with session key: ${config.CHAT_COMPLETION_SESSION_KEY}`
      );
      console.log(`[LLM] Using model: ${config.CHAT_COMPLETION_MODEL}`);
    }

    const response = await fetch(config.CHAT_COMPLETION_ENDPOINT_URL, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${config.CHAT_COMPLETION_APIKEY}`,
        "x-openclaw-session-key": config.CHAT_COMPLETION_SESSION_KEY,
      },
      body: JSON.stringify({
        model: config.CHAT_COMPLETION_MODEL,
        messages: [
          {
            role: "user",
            content: transcript,
          },
        ],
      }),
      signal: controller.signal,
    });

    if (!response.ok) {
      console.error(
        `[LLM] Chat completion request failed with status ${response.status}`
      );
      return null;
    }

    const data = (await response.json()) as ChatCompletionResponse;
    const llmResponse = data.choices?.[0]?.message?.content;

    if (!llmResponse) {
      console.error("[LLM] No content in chat completion response");
      return null;
    }

    return llmResponse;
  } catch (error) {
    // ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒ­ã‚°ãƒãƒ£ãƒ³ãƒãƒ«ã«è¨˜éŒ²
    if (error instanceof Error && error.name === "AbortError") {
      console.error("[LLM] Request timed out after 60 seconds");
      const cachedLogChannel = getCachedLogChannel();
      if (cachedLogChannel) {
        const timestamp = getJapaneseTimestamp();
        const timeoutMessage = `âš ï¸ **LLMã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ** â€” ${timestamp}\nLLMã‹ã‚‰ã®å¿œç­”ãŒ60ç§’ä»¥å†…ã«å¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚`;
        cachedLogChannel
          .send(timeoutMessage)
          .catch((sendError) =>
            console.error(
              "[LLM] Failed to send timeout message to channel:",
              sendError
            )
          );
      }
    } else {
      console.error("[LLM] Error sending chat completion request:", error);
    }
    return null;
  } finally {
    clearTimeout(timeoutId);
  }
}

/**
 * ãƒœã‚¤ã‚¹ãƒ­ã‚°ãƒãƒ£ãƒ³ãƒãƒ«ã«æ–‡å­—èµ·ã“ã—ã‚’æŠ•ç¨¿
 */
export async function sendTranscriptionToChannel(
  username: string,
  transcript: string
) {
  const cachedLogChannel = getCachedLogChannel();
  if (!cachedLogChannel || !transcript.trim()) return;

  try {
    const timestamp = getJapaneseTimestamp();
    const message = `ğŸ’¬ **${username}** â€” ${timestamp}\n${transcript}`;
    await cachedLogChannel.send(message);
    console.log(`[Transcription] ${username}: ${transcript}`);

    // LLMã«æ–‡å­—èµ·ã“ã—çµæœã‚’é€ä¿¡ã—ã¦å‡¦ç†ï¼ˆéåŒæœŸã§ä¸¦è¡Œå®Ÿè¡Œï¼‰
    (async () => {
      try {
        const llmResponse = await sendChatCompletionRequest(transcript);
        if (llmResponse) {
          const llmTimestamp = getJapaneseTimestamp();
          const llmMessage = `ğŸ¤– **LLMå¿œç­”** â€” ${llmTimestamp}\n${llmResponse}`;
          await cachedLogChannel.send(llmMessage);
          if (config.VERBOSE) {
            console.log(`[LLM] Response sent to channel for: ${transcript}`);
          }
        }
      } catch (error) {
        console.error("[LLM] Error processing and sending LLM response:", error);
        // ãƒ­ã‚°ãƒãƒ£ãƒ³ãƒãƒ«ã«ã‚¨ãƒ©ãƒ¼ã‚’é€šçŸ¥
        try {
          const timestamp = getJapaneseTimestamp();
          const errorMessage = `âŒ **LLMã‚¨ãƒ©ãƒ¼** â€” ${timestamp}\nLLMå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ${error instanceof Error ? error.message : String(error)}`;
          await cachedLogChannel.send(errorMessage);
        } catch (sendError) {
          console.error(
            "[LLM] Failed to send error message to channel:",
            sendError
          );
        }
      }
    })();
  } catch (error) {
    console.error("Error sending transcription:", error);
  }
}
