import { getCachedLogChannel } from "./state";
import { config } from "./config";

/**
 * æ—¥æœ¬æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
 */
export function getJapaneseTimestamp(): string {
  const now = new Date();
  return now.toLocaleString("ja-JP", {
    timeZone: "Asia/Tokyo",
    year: "numeric",
    month: "2-digit",
    day: "2-digit",
    hour: "2-digit",
    minute: "2-digit",
    second: "2-digit",
  });
}

/**
 * OpenAI chat completionäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
 */
async function sendChatCompletionRequest(
  transcript: string
): Promise<string | null> {
  // ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆURLã¨APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
  if (!config.CHAT_COMPLETION_ENDPOINT_URL || !config.CHAT_COMPLETION_APIKEY) {
    if (config.VERBOSE) {
      console.log(
        "[LLM] Chat completion endpoint or API key not configured, skipping LLM processing"
      );
    }
    return null;
  }

  try {
    const response = await fetch(config.CHAT_COMPLETION_ENDPOINT_URL, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${config.CHAT_COMPLETION_APIKEY}`,
      },
      body: JSON.stringify({
        model: "gpt-4", // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«åï¼ˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå´ã§ç„¡è¦–ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰
        messages: [
          {
            role: "user",
            content: transcript,
          },
        ],
      }),
    });

    if (!response.ok) {
      console.error(
        `[LLM] Chat completion request failed with status ${response.status}`
      );
      return null;
    }

    const data = (await response.json()) as {
      choices?: Array<{
        message?: {
          content?: string;
        };
      }>;
    };
    const llmResponse = data.choices?.[0]?.message?.content;

    if (!llmResponse) {
      console.error("[LLM] No content in chat completion response");
      return null;
    }

    return llmResponse;
  } catch (error) {
    console.error("[LLM] Error sending chat completion request:", error);
    return null;
  }
}

/**
 * ãƒœã‚¤ã‚¹ãƒ­ã‚°ãƒãƒ£ãƒ³ãƒãƒ«ã«æ–‡å­—èµ·ã“ã—ã‚’æŠ•ç¨¿
 */
export async function sendTranscriptionToChannel(
  username: string,
  transcript: string
) {
  const cachedLogChannel = getCachedLogChannel();
  if (!cachedLogChannel || !transcript.trim()) return;

  try {
    const timestamp = getJapaneseTimestamp();
    const message = `ğŸ’¬ **${username}** â€” ${timestamp}\n${transcript}`;
    await cachedLogChannel.send(message);
    console.log(`[Transcription] ${username}: ${transcript}`);

    // LLMã«æ–‡å­—èµ·ã“ã—çµæœã‚’é€ä¿¡ã—ã¦å‡¦ç†ï¼ˆéåŒæœŸã§ä¸¦è¡Œå®Ÿè¡Œï¼‰
    sendChatCompletionRequest(transcript)
      .then((llmResponse) => {
        if (llmResponse) {
          const llmTimestamp = getJapaneseTimestamp();
          const llmMessage = `ğŸ¤– **LLMå¿œç­”** â€” ${llmTimestamp}\n${llmResponse}`;
          return cachedLogChannel.send(llmMessage);
        }
      })
      .then(() => {
        if (config.VERBOSE) {
          console.log(`[LLM] Response sent to channel for: ${transcript}`);
        }
      })
      .catch((error) => {
        console.error("[LLM] Error processing LLM response:", error);
      });
  } catch (error) {
    console.error("Error sending transcription:", error);
  }
}
